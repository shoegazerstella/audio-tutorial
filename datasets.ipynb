{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other examples of dataset:\n",
    "* [torchvision](https://github.com/pytorch/vision/blob/master/torchvision/datasets/mnist.py)\n",
    "* generator for [tarballs](https://docs.python.org/3/library/tarfile.html#tarfile.TarFile.extractfile) and [zip](https://docs.python.org/3/library/zipfile.html#zipfile.ZipFile.open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision \n",
    "import torchaudio\n",
    "\n",
    "import os\n",
    "import random\n",
    "from functools import reduce, partial\n",
    "from warnings import warn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(URL):\n",
    "    r = requests.get(URL)\n",
    "    file_like_object = io.BytesIO(r.content)\n",
    "    tar = tarfile.open(fileobj=file_like_object)\n",
    "    d = {}\n",
    "    for member in tar.getmembers():\n",
    "        if member.isfile() and member.name.endswith('csv'):\n",
    "            k = 'train' if 'train' in member.name else 'test'\n",
    "            d[k] = tar.extractfile(member)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cache:\n",
    "    \"\"\"\n",
    "    Wrap a generator so that, whenever a new item is returned, it is saved to disk in a pickle.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, generator, location):\n",
    "        self.generator = generator\n",
    "        self.location = location\n",
    "\n",
    "        self._id = id(self)\n",
    "        self._cache = []\n",
    "        self._internal_index = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._internal_index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._internal_index < len(self):\n",
    "            item = self[self._internal_index]\n",
    "        else:\n",
    "            item = next(self.generator)\n",
    "        \n",
    "            file = str(self._id) + \"-\" + str(len(self))\n",
    "            file = os.path.join(self.location, file)\n",
    "            self._cache.append(file)\n",
    "        \n",
    "            os.makedirs(self.location, exist_ok=True)\n",
    "            with open(file, 'wb') as file:\n",
    "                pickle.dump(item, file)\n",
    "\n",
    "        self._internal_index += 1\n",
    "        return item\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        file = self._cache[index]\n",
    "        with open(file, 'rb') as file:\n",
    "            item = pickle.load(file)\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return length of cache\n",
    "        return len(self._cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose(*funcs):\n",
    "    \"\"\"\n",
    "    Compose multiple generator.\n",
    "    \"\"\"\n",
    "    return lambda x: reduce(lambda f, g: g(f), list(funcs), x)\n",
    "\n",
    "\n",
    "def download(urls, root_path):\n",
    "    \"\"\"\n",
    "    Download each url to root_path.\n",
    "    \n",
    "    Input: url generator, folder inside archive\n",
    "    Output: downloaded archive, folder inside archive\n",
    "    \"\"\"\n",
    "    for url, folder in urls:\n",
    "        # torchvision.datasets.utils.download_url(url, root_path)\n",
    "        file = os.path.join(root_path, os.path.basename(url))\n",
    "        yield file, folder\n",
    "    \n",
    "    \n",
    "def extract(files):\n",
    "    \"\"\"\n",
    "    Extract each archive to their respective folder.\n",
    "    \n",
    "    Input: (url, folder name inside archive) generator\n",
    "    Output: path to inside archive\n",
    "    \"\"\"\n",
    "    for file, folder in files:\n",
    "        # torchvision.datasets.utils.extract_archive(file)\n",
    "        path = os.path.dirname(file)\n",
    "        path = os.path.join(path, folder)\n",
    "        yield path\n",
    "          \n",
    "            \n",
    "def walk(paths, extension):\n",
    "    \"\"\"\n",
    "    Walk inside a path recursively to find all files with given extension.\n",
    "    \n",
    "    Input: path\n",
    "    Output: path, file name identifying a row of data\n",
    "    \"\"\"\n",
    "    for path in paths:\n",
    "        for dp, dn, fn in os.walk(path):\n",
    "            for f in fn:\n",
    "                if extension in f:\n",
    "                    yield path, f\n",
    "\n",
    "                    \n",
    "def shuffle(generator):\n",
    "    \"\"\"\n",
    "    Shuffle the order of a generator.\n",
    "    \n",
    "    Input: generator\n",
    "    Output: generator\n",
    "    \"\"\"\n",
    "\n",
    "    # Need to load the whole list in memory\n",
    "    generator = list(generator)\n",
    "    # print(len(generator))\n",
    "    random.shuffle(generator)\n",
    "    for g in generator:\n",
    "        yield g\n",
    "\n",
    "        \n",
    "def filtering(fileids, reference):\n",
    "    \"\"\"\n",
    "    Skip fileids that are not present in given reference file.\n",
    "    \n",
    "    Output: (path, file) generator, reference file\n",
    "    Output: path, file\n",
    "    \"\"\"\n",
    "    \n",
    "    path_old = \"\"\n",
    "    \n",
    "    for path, fileid in fileids:\n",
    "        \n",
    "        if path != path_old:\n",
    "            # Check if same path to avoid reloading the file constantly\n",
    "            ref = os.path.join(path, reference)\n",
    "            with open(ref) as ref:\n",
    "                r = \"\".join(ref.readlines())\n",
    "\n",
    "        # It would be more efficient to loop through the reference file instead\n",
    "        if fileid in r:\n",
    "            yield path, fileid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YesNo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['1', '1', '1', '0', '1', '0', '1', '0'],\n",
       " tensor([[ 0.0016,  0.0017,  0.0016,  ..., -0.0016, -0.0010, -0.0002]]),\n",
       " 8000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_yesno(fileids):\n",
    "    \"\"\"\n",
    "    Load data corresponding to each YESNO fileids.\n",
    "    \n",
    "    Input: path, file name identifying a row of data\n",
    "    Output: label, waveform, sample_rate\n",
    "    \"\"\"\n",
    "    \n",
    "    extension = \".wav\"\n",
    "    for path, fileid in fileids:\n",
    "        file = os.path.join(path, fileid)\n",
    "        waveform, sample_rate = torchaudio.load(file)\n",
    "        label = os.path.basename(fileid).split(\".\")[0].split(\"_\")\n",
    "    \n",
    "        yield label, waveform, sample_rate\n",
    "        \n",
    "\n",
    "def YESNO(root):\n",
    "    \"\"\"\n",
    "    Cache a pipeline loading YESNO.\n",
    "    \"\"\"\n",
    "    \n",
    "    url = [\n",
    "        (\"http://www.openslr.org/resources/1/waves_yesno.tar.gz\", \"waves_yesno\")\n",
    "    ]\n",
    "     \n",
    "    pipeline = compose(\n",
    "        partial(download, root_path=root),\n",
    "        extract,\n",
    "        partial(walk, extension=\".wav\"),\n",
    "        shuffle,\n",
    "        load_yesno,\n",
    "    )\n",
    "    \n",
    "    return Cache(pipeline(url), \"tmp/\")\n",
    "\n",
    "\n",
    "data = YESNO(\"/Users/vincentqb/yesnotest\")\n",
    "\n",
    "next(data)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VCTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('p284_077',\n",
       " 'Even the one she loved.\\n',\n",
       " tensor([[ 0.0007,  0.0013,  0.0009,  ..., -0.0007, -0.0012, -0.0011]]),\n",
       " 48000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_vctk(fileids):\n",
    "    \"\"\"\n",
    "    Load data corresponding to each VCTK fileids.\n",
    "\n",
    "    Input: path, file name identifying a row of data\n",
    "    Output: id, content, waveform, sample_rate\n",
    "    \"\"\"\n",
    "    \n",
    "    txt_folder = \"txt\"\n",
    "    txt_extension = \".txt\"\n",
    "    \n",
    "    audio_folder = \"wav48\"\n",
    "    audio_extension = \".wav\"\n",
    "    \n",
    "    for path, fileid in fileids:\n",
    "        \n",
    "        fileid = os.path.basename(fileid).split(\".\")[0]\n",
    "        folder = fileid.split(\"_\")[0]\n",
    "        txt_file = os.path.join(path, txt_folder, folder, fileid + txt_extension)        \n",
    "        audio_file = os.path.join(path, audio_folder, folder, fileid + audio_extension)        \n",
    "        \n",
    "        try:\n",
    "            with open(txt_file) as txt_file:\n",
    "                content = txt_file.readlines()[0]\n",
    "        except FileNotFoundError:\n",
    "            warn(\"Translation not found for {}\".format(audio_file))\n",
    "            # warn(\"File not found: {}\".format(txt_file))\n",
    "            continue\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "        \n",
    "        yield fileid, content, waveform, sample_rate\n",
    "        \n",
    "        \n",
    "def VCTK(root):\n",
    "    \"\"\"\n",
    "    Cache a pipeline loading VCTK.\n",
    "    \"\"\"\n",
    "    \n",
    "    url = [\n",
    "        ('http://homepages.inf.ed.ac.uk/jyamagis/release/VCTK-Corpus.tar.gz', \"VCTK-Corpus/\")\n",
    "    ]\n",
    "    \n",
    "    pipeline = compose(\n",
    "        partial(download, root_path=root),\n",
    "        extract,\n",
    "        partial(walk, extension=\".wav\"),\n",
    "        shuffle,\n",
    "        load_vctk,\n",
    "    )\n",
    "    \n",
    "    return Cache(pipeline(url), \"tmp/\")\n",
    "\n",
    "\n",
    "data = VCTK(\"/Users/vincentqb/vctktest/\")\n",
    "\n",
    "next(data)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LibriSpeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3853-163249-0032',\n",
       " tensor([[-0.0011, -0.0030, -0.0018,  ...,  0.0016, -0.0002, -0.0073]]),\n",
       " 16000,\n",
       " 'CAN YOU REMEMBER WHAT HEPSEY TOLD US AND CALL THEM POOR LONG SUFFERIN CREETERS NAMES')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_librispeech(fileids):\n",
    "    \"\"\"\n",
    "    Load data corresponding to each LIBRISPEECH fileids.\n",
    "    \n",
    "    Input: path, file name identifying a row of data\n",
    "    Output: id, waveform, sample_rate, translation\n",
    "    \"\"\"\n",
    "    \n",
    "    text_extension = \".trans.txt\"\n",
    "    audio_extension = \".flac\"\n",
    "    for data_path, fileid in fileids:\n",
    "        fileid = os.path.basename(fileid).split(\".\")[0]\n",
    "        folder1, folder2, file = fileid.split(\"-\")\n",
    "        file_text = folder1 + \"-\" + folder2 + text_extension\n",
    "        file_text = os.path.join(data_path, folder1, folder2, file_text)\n",
    "        file_audio = folder1 + \"-\"+ folder2 + \"-\" + file + audio_extension\n",
    "        file_audio = os.path.join(data_path, folder1, folder2, file_audio)\n",
    "        waveform, sample_rate = torchaudio.load(file_audio)\n",
    "        \n",
    "        found = False\n",
    "        for line in open(file_text):\n",
    "            fileid_text, content = line.strip().split(\" \", 1)\n",
    "            if fileid == fileid_text:\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            from warnings import warn\n",
    "            warn(\"Translation not found for {}.\".format(fileid))\n",
    "            continue\n",
    "\n",
    "        yield fileid, waveform, sample_rate, content\n",
    "        \n",
    "\n",
    "def LIBRISPEECH(root, selection=\"dev-clean\"):\n",
    "    \"\"\"\n",
    "    Cache a pipeline loading LIBRISPEECH.\n",
    "    \"\"\"\n",
    "    \n",
    "    # http://www.openslr.org/resources/12/dev-clean.tar.gz\n",
    "    # http://www.openslr.org/resources/12/test-clean.tar.gz\n",
    "    # http://www.openslr.org/resources/12/test-other.tar.gz\n",
    "    # http://www.openslr.org/resources/12/train-clean-100.tar.gz\n",
    "    # http://www.openslr.org/resources/12/train-clean-360.tar.gz\n",
    "    # http://www.openslr.org/resources/12/train-other-500.tar.gz\n",
    "\n",
    "    selections = [\n",
    "        \"dev-clean\",\n",
    "        \"test-clean\",\n",
    "        \"test-other\",\n",
    "        \"train-clean-100\",\n",
    "        \"train-clean-360\",\n",
    "        \"train-other-500\"\n",
    "    ]\n",
    "        \n",
    "    base = \"http://www.openslr.org/resources/12/\"\n",
    "    url = [\n",
    "        (os.path.join(base, selection + \".tar.gz\"), os.path.join(\"LibriSpeech\", selection))\n",
    "    ]\n",
    "     \n",
    "    pipeline = compose(\n",
    "        partial(download, root_path=root),\n",
    "        extract,\n",
    "        partial(walk, extension=\".flac\"),\n",
    "        shuffle,\n",
    "        load_librispeech,\n",
    "    )\n",
    "\n",
    "    return Cache(pipeline(url), \"tmp/\")\n",
    "\n",
    "\n",
    "data = LIBRISPEECH(\"/Users/vincentqb/librispeechtest/\")\n",
    "\n",
    "next(data)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CommonVoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bb10e83bdf015da18144f427509d8cb56cfa4884527dc0cb3da927c845b733e48d3c451ae9538723b747fd6e34b15a863635e71b09a7611b7484f09e4cd109be',\n",
       " 'common_voice_tt_17759554.mp3',\n",
       " 'Авыл башлыкларына исем ошамады булса кирәк.',\n",
       " '2',\n",
       " '0',\n",
       " 'thirties',\n",
       " 'male',\n",
       " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -2.9039e-05,\n",
       "          -1.0319e-06,  2.4986e-05]]),\n",
       " 48000]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_commonvoice(fileids, tsv):\n",
    "    \"\"\"\n",
    "    Load data corresponding to each COMMONVOICE fileids.\n",
    "    \n",
    "    Input: path, file name identifying a row of data\n",
    "    Output: client_id, path, sentence, up_votes, down_votes, age, gender, accent, waveform, sample_rate\n",
    "    \"\"\"\n",
    "    \n",
    "    for path, fileid in fileids:\n",
    "        filename = os.path.join(path, \"clips\", fileid)\n",
    "        tsv = os.path.join(path, tsv)\n",
    "        \n",
    "        found = False\n",
    "        for line in open(tsv):\n",
    "            if fileid in line:\n",
    "                # client_id, path, sentence, up_votes, down_votes, age, gender, accent\n",
    "                line = line.strip().split(\"\\t\")\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            continue\n",
    "\n",
    "        # waveform, sample_rate\n",
    "        output = torchaudio.load(filename)    \n",
    "\n",
    "        line.extend(output)\n",
    "        yield line\n",
    "        \n",
    "\n",
    "def COMMONVOICE(root, language=\"tatar\", tsv=\"train.tsv\"):\n",
    "    \"\"\"\n",
    "    Cache a pipeline loading COMMONVOICE.\n",
    "    \"\"\"\n",
    "    \n",
    "    web = \"https://voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4.s3.amazonaws.com/cv-corpus-3/\"\n",
    "\n",
    "    languages = {\n",
    "        \"tatar\": \"tt\",\n",
    "        \"english\": \"en\",\n",
    "        \"german\": \"de\",\n",
    "        \"french\": \"fr\",\n",
    "        \"welsh\": \"cy\",\n",
    "        \"breton\": \"br\",\n",
    "        \"chuvash\": \"cv\",\n",
    "        \"turkish\": \"tr\",\n",
    "        \"kyrgyz\": \"ky\",\n",
    "        \"irish\": \"ga-IE\",\n",
    "        \"kabyle\": \"kab\",\n",
    "        \"catalan\": \"ca\",\n",
    "        \"taiwanese\": \"zh-TW\",\n",
    "        \"slovenian\": \"sl\",\n",
    "        \"italian\": \"it\",\n",
    "        \"dutch\": \"nl\",\n",
    "        \"hakha chin\": \"cnh\",\n",
    "        \"esperanto\": \"eo\",\n",
    "        \"estonian\": \"et\",\n",
    "        \"persian\": \"fa\",\n",
    "        \"basque\": \"eu\",\n",
    "        \"spanish\": \"es\",\n",
    "        \"chinese\": \"zh-CN\",\n",
    "        \"mongolian\": \"mn\",\n",
    "        \"sakha\": \"sah\",\n",
    "        \"dhivehi\": \"dv\",\n",
    "        \"kinyarwanda\": \"rw\",\n",
    "        \"swedish\": \"sv-SE\",\n",
    "        \"russian\": \"ru\",\n",
    "    }\n",
    "\n",
    "    url = web + languages[language] + \".tar.gz\"\n",
    "    url = [(url, \"\")]\n",
    "     \n",
    "    pipeline = compose(\n",
    "        partial(download, root_path=root),\n",
    "        extract,\n",
    "        partial(walk, extension=\".mp3\"),\n",
    "        # partial(filtering, reference=tsv),\n",
    "        shuffle,\n",
    "        partial(load_commonvoice, tsv=tsv),\n",
    "    )\n",
    "    \n",
    "    return Cache(pipeline(url), \"tmp/\")\n",
    "\n",
    "\n",
    "data = COMMONVOICE(\"/Users/vincentqb/commonvoicetest/\")\n",
    "\n",
    "next(data)\n",
    "data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
