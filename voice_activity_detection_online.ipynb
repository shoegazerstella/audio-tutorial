{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [a simple but efficient real-time voice activity detection\n",
    "algorithm](https://www.eurasip.org/Proceedings/Eusipco/Eusipco2009/contents/papers/1569192958.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voice sound\n",
    "dataset = torchaudio.datasets.YESNO(\"~/yesno\")\n",
    "waveform = dataset[0][0]\n",
    "sample_rate = 8 * 10**3\n",
    "\n",
    "# Train sound\n",
    "# filename = \"_static/img/steam-train-whistle-daniel_simon-converted-from-mp3.wav\"\n",
    "# waveform, sample_rate = torchaudio.load(filename)\n",
    "\n",
    "# Convert to mono    \n",
    "waveform = waveform.mean(0).view(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three criteria to decide if a frame contains speech: energy, most dominant frequency, and spectral flatness. If any two of those are higher than a minimum plus a threshold, then the frame contains speech. In the offline case, the list of frames is postprocessed to remove too short silence and speech sequences. In the online case here, inertia is added before switching from speech to silence or vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectral_flatness(frame):\n",
    "    EPSILON = 0.01\n",
    "    n = frame.nonzero().size(0)\n",
    "    geometric_mean = torch.exp((EPSILON + frame).log().mean(-1)) - EPSILON\n",
    "    arithmetic_mean = frame.mean(-1)\n",
    "    return -10 * torch.log10(EPSILON + geometric_mean/arithmetic_mean)\n",
    "\n",
    "def compute_energy(frame):\n",
    "    return frame.pow(2).sum(-1)\n",
    "\n",
    "\n",
    "class VoiceActivityDetection(object):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.num_init_frames = 30\n",
    "        self.ignore_silent_count = 10\n",
    "        self.ignore_speech_count = 5\n",
    "\n",
    "        self.energy_prim_thresh = 40\n",
    "        self.frequency_prim_thresh = 5\n",
    "        self.spectral_flatness_prim_thresh = 3\n",
    "        \n",
    "        self.ignore_silent_count = 4\n",
    "        self.ignore_speech_count = 1\n",
    "\n",
    "        self.speech_mark = True\n",
    "        self.silence_mark = False\n",
    "\n",
    "        self.silent_count = 0\n",
    "        self.speech_count = 0\n",
    "        self.n = 0\n",
    "        \n",
    "        self.energy_list = []\n",
    "        self.frequency_list = []\n",
    "        self.spectral_flatness_list = []\n",
    "\n",
    "    def iter(self, frame):\n",
    "\n",
    "        EPSILON = 1.\n",
    "        frame_fft = torch.rfft(frame, 1)\n",
    "        amplitudes = torchaudio.functional.complex_norm(frame_fft)\n",
    "\n",
    "        # Compute frame energy\n",
    "        energy = compute_energy(frame)\n",
    "    \n",
    "        # Most dominant frequency component\n",
    "        frequency = amplitudes.argmax()\n",
    "    \n",
    "        # Spectral flatness measure\n",
    "        spectral_flatness = compute_spectral_flatness(amplitudes)\n",
    "        \n",
    "        self.energy_list.append(energy)\n",
    "        self.frequency_list.append(frequency)\n",
    "        self.spectral_flatness_list.append(spectral_flatness)\n",
    "    \n",
    "        if self.n == 0:\n",
    "            self.min_energy = energy\n",
    "            self.min_frequency = frequency\n",
    "            self.min_spectral_flatness = spectral_flatness\n",
    "        elif self.n < self.num_init_frames:\n",
    "            self.min_energy = min(energy, self.min_energy)\n",
    "            self.min_frequency = min(frequency, self.min_frequency)\n",
    "            self.min_spectral_flatness = min(spectral_flatness, self.min_spectral_flatness)\n",
    "    \n",
    "        self.n +=1\n",
    "        \n",
    "        thresh_energy = self.energy_prim_thresh * torch.log(EPSILON + self.min_energy)\n",
    "        thresh_frequency = self.frequency_prim_thresh\n",
    "        thresh_spectral_flatness = self.spectral_flatness_prim_thresh\n",
    "    \n",
    "        # Check all three conditions\n",
    "    \n",
    "        counter = 0\n",
    "        if energy - self.min_energy >= thresh_energy:\n",
    "            counter += 1\n",
    "        if frequency - self.min_frequency >= thresh_frequency: \n",
    "            counter += 1\n",
    "        if spectral_flatness - self.min_spectral_flatness >= thresh_spectral_flatness: \n",
    "            counter += 1\n",
    "        \n",
    "        # Detection\n",
    "        if counter > 1:\n",
    "            # Speech detected\n",
    "            self.speech_count += 1\n",
    "            # Inertia against switching\n",
    "            if self.n >= self.num_init_frames and self.speech_count <= self.ignore_speech_count:\n",
    "                # Too soon to change\n",
    "                return self.silence_mark\n",
    "            else:\n",
    "                self.silent_count = 0\n",
    "                return self.speech_mark\n",
    "        else:\n",
    "            # Silence detected\n",
    "            self.min_energy = ((self.silent_count * self.min_energy) + energy) / (self.silent_count + 1)\n",
    "            self.silent_count += 1\n",
    "            # Inertia against switching\n",
    "            if self.n >= self.num_init_frames and self.silent_count <= self.ignore_silent_count:\n",
    "                # Too soon to change\n",
    "                return self.speech_mark\n",
    "            else:\n",
    "                self.speech_count = 0\n",
    "                return self.silence_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_waveform(speech_frames, waveform):\n",
    "    z = torch.zeros(waveform.shape, dtype=waveform.dtype, device=waveform.device)\n",
    "\n",
    "    # Convert to waveform\n",
    "    n_frames = len(speech_frames)\n",
    "    for i in range(n_frames):\n",
    "        z[0, i*frame_size:(i+1)*frame_size] = int(speech_frames[i])\n",
    "    \n",
    "    # Extend with what was last detected\n",
    "    z[0, n_frames*frame_size:-1] = int(speech_frames[-1])\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine number of frames and frame size\n",
    "\n",
    "frame_time = 10**-2\n",
    "\n",
    "frame_size = int(math.ceil(sample_rate*frame_time))\n",
    "waveform_length = waveform.size()[-1]\n",
    "num_of_frames = math.floor(waveform_length/frame_size)\n",
    "\n",
    "# Iterate VAD\n",
    "\n",
    "vad = VoiceActivityDetection()\n",
    "speech_frames = []\n",
    "\n",
    "for n in range(num_of_frames):\n",
    "    waveform_short = waveform[0, n*frame_size:(n+1)*frame_size]\n",
    "    speech_frames.append(vad.iter(waveform_short))\n",
    "\n",
    "z = reconstruct_waveform(speech_frames, waveform)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(vad.energy_list)\n",
    "plt.figure()\n",
    "plt.plot(vad.frequency_list)\n",
    "plt.figure()\n",
    "plt.plot(vad.spectral_flatness_list)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(waveform.t())\n",
    "plt.plot(waveform.abs().max()*z.t())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
