{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [a simple but efficient real-time voice activity detection\n",
    "algorithm](https://www.eurasip.org/Proceedings/Eusipco/Eusipco2009/contents/papers/1569192958.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import math\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "from six.moves import queue\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voice sound\n",
    "dataset = torchaudio.datasets.YESNO(\"~/yesno\")\n",
    "waveform = dataset[0][0]\n",
    "sample_rate = 8 * 10**3\n",
    "\n",
    "# Train sound\n",
    "# filename = \"_static/img/steam-train-whistle-daniel_simon-converted-from-mp3.wav\"\n",
    "# waveform, sample_rate = torchaudio.load(filename)\n",
    "\n",
    "# Convert to mono    \n",
    "waveform = waveform.mean(0).view(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three criteria to decide if a frame contains speech: energy, most dominant frequency, and spectral flatness. If any two of those are higher than a minimum plus a threshold, then the frame contains speech. In the offline case, the list of frames is postprocessed to remove too short silence and speech sequences. In the online case here, inertia is added before switching from speech to silence or vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectral_flatness(frame):\n",
    "    EPSILON = 0.01\n",
    "    n = frame.nonzero().size(0)\n",
    "    geometric_mean = torch.exp((EPSILON + frame).log().mean(-1)) - EPSILON\n",
    "    arithmetic_mean = frame.mean(-1)\n",
    "    return -10 * torch.log10(EPSILON + geometric_mean/arithmetic_mean)\n",
    "\n",
    "\n",
    "def compute_energy(frame):\n",
    "    return frame.pow(2).sum(-1)\n",
    "\n",
    "\n",
    "class VoiceActivityDetection(object):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.num_init_frames = 30\n",
    "        self.ignore_silent_count = 10\n",
    "        self.ignore_speech_count = 5\n",
    "\n",
    "        self.energy_prim_thresh = 40\n",
    "        self.frequency_prim_thresh = 5\n",
    "        self.spectral_flatness_prim_thresh = 3\n",
    "        \n",
    "        self.ignore_silent_count = 4\n",
    "        self.ignore_speech_count = 1\n",
    "\n",
    "        self.speech_mark = True\n",
    "        self.silence_mark = False\n",
    "\n",
    "        self.silent_count = 0\n",
    "        self.speech_count = 0\n",
    "        self.n = 0\n",
    "        \n",
    "        self.energy_list = []\n",
    "        self.frequency_list = []\n",
    "        self.spectral_flatness_list = []\n",
    "\n",
    "    def iter(self, frame):\n",
    "\n",
    "        EPSILON = 1.\n",
    "        frame_fft = torch.rfft(frame, 1)\n",
    "        amplitudes = torchaudio.functional.complex_norm(frame_fft)\n",
    "\n",
    "        # Compute frame energy\n",
    "        energy = compute_energy(frame)\n",
    "    \n",
    "        # Most dominant frequency component\n",
    "        frequency = amplitudes.argmax()\n",
    "    \n",
    "        # Spectral flatness measure\n",
    "        spectral_flatness = compute_spectral_flatness(amplitudes)\n",
    "        \n",
    "        self.energy_list.append(energy)\n",
    "        self.frequency_list.append(frequency)\n",
    "        self.spectral_flatness_list.append(spectral_flatness)\n",
    "    \n",
    "        if self.n == 0:\n",
    "            self.min_energy = energy\n",
    "            self.min_frequency = frequency\n",
    "            self.min_spectral_flatness = spectral_flatness\n",
    "        elif self.n < self.num_init_frames:\n",
    "            self.min_energy = min(energy, self.min_energy)\n",
    "            self.min_frequency = min(frequency, self.min_frequency)\n",
    "            self.min_spectral_flatness = min(spectral_flatness, self.min_spectral_flatness)\n",
    "    \n",
    "        self.n +=1\n",
    "        \n",
    "        thresh_energy = self.energy_prim_thresh * torch.log(EPSILON + self.min_energy)\n",
    "        thresh_frequency = self.frequency_prim_thresh\n",
    "        thresh_spectral_flatness = self.spectral_flatness_prim_thresh\n",
    "    \n",
    "        # Check all three conditions\n",
    "    \n",
    "        counter = 0\n",
    "        if energy - self.min_energy >= thresh_energy:\n",
    "            counter += 1\n",
    "        if frequency - self.min_frequency >= thresh_frequency: \n",
    "            counter += 1\n",
    "        if spectral_flatness - self.min_spectral_flatness >= thresh_spectral_flatness: \n",
    "            counter += 1\n",
    "        \n",
    "        # Detection\n",
    "        if counter > 1:\n",
    "            # Speech detected\n",
    "            self.speech_count += 1\n",
    "            # Inertia against switching\n",
    "            if self.n >= self.num_init_frames and self.speech_count <= self.ignore_speech_count:\n",
    "                # Too soon to change\n",
    "                return self.silence_mark\n",
    "            else:\n",
    "                self.silent_count = 0\n",
    "                return self.speech_mark\n",
    "        else:\n",
    "            # Silence detected\n",
    "            self.min_energy = ((self.silent_count * self.min_energy) + energy) / (self.silent_count + 1)\n",
    "            self.silent_count += 1\n",
    "            # Inertia against switching\n",
    "            if self.n >= self.num_init_frames and self.silent_count <= self.ignore_silent_count:\n",
    "                # Too soon to change\n",
    "                return self.speech_mark\n",
    "            else:\n",
    "                self.speech_count = 0\n",
    "                return self.silence_mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MicrophoneStream(object):\n",
    "    \"\"\"Opens a recording stream as a generator yielding the audio chunks.\"\"\"\n",
    "    def __init__(self, device=None, rate=22050, chunk=2205):\n",
    "        \"\"\"\n",
    "        The 22050 is the librosa default, which is what our models were\n",
    "        trained on.  The ratio of [chunk / rate] is the amount of time between\n",
    "        audio samples - for example, with these defaults,\n",
    "        an audio fragment will be processed every tenth of a second.\n",
    "        \"\"\"\n",
    "        self._rate = rate\n",
    "        self._chunk = chunk\n",
    "        self._device = device\n",
    "\n",
    "        # Create a thread-safe buffer of audio data\n",
    "        self._buff = queue.Queue()\n",
    "        self.closed = True\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._audio_interface = pyaudio.PyAudio()\n",
    "        self._audio_stream = self._audio_interface.open(\n",
    "            #format=pyaudio.paInt16,\n",
    "            format=pyaudio.paFloat32,\n",
    "            # The API currently only supports 1-channel (mono) audio\n",
    "            # https://goo.gl/z757pE\n",
    "            channels=1, rate=self._rate,\n",
    "            input=True, frames_per_buffer=self._chunk,\n",
    "            input_device_index=self._device,\n",
    "            # Run the audio stream asynchronously to fill the buffer object.\n",
    "            # This is necessary so that the input device's buffer doesn't\n",
    "            # overflow while the calling thread makes network requests, etc.\n",
    "            stream_callback=self._fill_buffer,\n",
    "        )\n",
    "\n",
    "        self.closed = False\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self._audio_stream.stop_stream()\n",
    "        self._audio_stream.close()\n",
    "        self.closed = True\n",
    "        # Signal the generator to terminate so that the client's\n",
    "        # streaming_recognize method will not block the process termination.\n",
    "        self._buff.put(None)\n",
    "        self._audio_interface.terminate()\n",
    "\n",
    "    def _fill_buffer(self, in_data, frame_count, time_info, status_flags):\n",
    "        \"\"\"Continuously collect data from the audio stream, into the buffer.\"\"\"\n",
    "        self._buff.put(in_data)\n",
    "        return None, pyaudio.paContinue\n",
    "\n",
    "    def generator(self):\n",
    "        while not self.closed:\n",
    "            # Use a blocking get() to ensure there's at least one chunk of\n",
    "            # data, and stop iteration if the chunk is None, indicating the\n",
    "            # end of the audio stream.\n",
    "            chunk = self._buff.get()\n",
    "            if chunk is None:\n",
    "                return\n",
    "            data = [chunk]\n",
    "\n",
    "            # Now consume whatever other data's still buffered.\n",
    "            while True:\n",
    "                try:\n",
    "                    chunk = self._buff.get(block=False)\n",
    "                    if chunk is None:\n",
    "                        return\n",
    "                    data.append(chunk)\n",
    "                except queue.Empty:\n",
    "                    break\n",
    "\n",
    "            ans = np.fromstring(b''.join(data), dtype=np.float32)\n",
    "            # yield uniform-sized chunks\n",
    "            ans = np.split(ans, np.shape(ans)[0] / self._chunk)\n",
    "            # Resample the audio to 22050, librosa default\n",
    "            for chunk in ans:\n",
    "                yield librosa.core.resample(chunk, self._rate, 22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate VAD\n",
    "\n",
    "vad = VoiceActivityDetection()\n",
    "speech_frames = []\n",
    "chunks = []\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "m = .2\n",
    "ax.set_ylim(-m,m)\n",
    "\n",
    "min_to_cumulate = 20  # 2 seconds, with defaults\n",
    "max_to_cumulate = 100  # 10 seconds with defaults\n",
    "precumulate = 5\n",
    "\n",
    "max_to_visualize = 100\n",
    "\n",
    "cumulated = []\n",
    "precumulated = deque(maxlen=precumulate)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "with MicrophoneStream() as stream:\n",
    "    audio_generator = stream.generator()\n",
    "    chunk_length = stream._chunk\n",
    "    waveform = torch.zeros(max_to_visualize*chunk_length)\n",
    "    speechform = torch.zeros(max_to_visualize*chunk_length)\n",
    "    try:\n",
    "        for chunk in audio_generator:\n",
    "            \n",
    "            # Is speech?\n",
    "\n",
    "            chunk = torch.tensor(chunk)\n",
    "            is_speech = vad.iter(chunk)\n",
    "            \n",
    "            # Cumulate speech\n",
    "            \n",
    "            if is_speech or cumulated:\n",
    "                cumulated.append(chunk)\n",
    "            else:\n",
    "                precumulated.append(chunk)\n",
    "            \n",
    "            if (not is_speech and len(cumulated) >= min_to_cumulate) or (len(cumulated) > max_to_cumulate):\n",
    "                z = torch.cat(list(precumulated) + cumulated, -1)\n",
    "                print(\"RUN PYSPEECH\")\n",
    "                print(z)\n",
    "                cumulated = []\n",
    "                precumulated = deque(maxlen=precumulate)\n",
    "            \n",
    "            # continue\n",
    "            # Plot\n",
    "            \n",
    "            waveform[:-chunk_length] = waveform[chunk_length:]\n",
    "            waveform[-chunk_length:] = chunk\n",
    "            speechform[:-chunk_length] = speechform[chunk_length:]\n",
    "            speechform[-chunk_length:] = int(is_speech)\n",
    "\n",
    "            if ax.lines:\n",
    "                ax.lines[0].set_ydata(waveform)\n",
    "                ax.lines[1].set_ydata(.95*m*speechform)\n",
    "                ax.lines[2].set_ydata(-.95*m*speechform)\n",
    "            else:\n",
    "                ax.plot(waveform)\n",
    "                ax.plot(.95*m*speechform, color=colors[1], linewidth=2)\n",
    "                ax.plot(-.95*m*speechform, color=colors[1], linewidth=2)\n",
    "            fig.canvas.draw()\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
